{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas para a análise dos abstracts\n",
        "\n",
        "!pip install pandas nltk wordcloud matplotlib yake"
      ],
      "metadata": {
        "id": "m2kXzJHlupil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "QcAfOgoCuOJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo download dos dados\n",
        "\n",
        "!wget https://raw.githubusercontent.com/brasileiras-pln/analise-acl-2023/main/abstracts_acl2023.csv"
      ],
      "metadata": {
        "id": "gXPoSr73i3G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo os dados em uma planilha do Pandas\n",
        "\n",
        "data_file = \"abstracts_acl2023.csv\"\n",
        "df = pd.read_csv(data_file, header=None, names=['Abstract'], delimiter=\";\")"
      ],
      "metadata": {
        "id": "T7Ezp6fkutn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo os trabalhos que não possuem abstract\n",
        "\n",
        "df['Abstract'].fillna(\"\", inplace=True)\n",
        "df = df[df['Abstract'] != \"\"]"
      ],
      "metadata": {
        "id": "M2q93JhCkB6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantidade de trabalhos com abstract presente:\n",
        "\n",
        "trabalhos_abstract = df.shape[0]\n",
        "print(f\"Trabalhos com abstract: {trabalhos_abstract}\")"
      ],
      "metadata": {
        "id": "rvPRBfkGuzYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrando os trabalhos que possuem o termo \"bias\"\n",
        "\n",
        "df = df[df['Abstract'].str.contains(\"bias\")]"
      ],
      "metadata": {
        "id": "cxBFX953jzBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantidade de trabalhos que possuem o termo \"bias\"\n",
        "\n",
        "trabalhos_bias = df.shape[0]\n",
        "print(f\"Trabalhos que possuem o termo 'bias': {trabalhos_bias}\")"
      ],
      "metadata": {
        "id": "xNb42jb1kgSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Porcentagem de trabalhos que possuem o termo \"bias\"\n",
        "\n",
        "print(f\"Porcentagem de trabalhos que possuem o termo 'bias': {(trabalhos_bias/trabalhos_abstract*100):.1f}%\")"
      ],
      "metadata": {
        "id": "NjbSqPXank6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Tokenização e remoção de stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        words = nltk.word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "        return ' '.join(words)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "df['Processed_Abstract'] = df['Abstract'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "D4e3wuA1vNq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "5WobEM19vTQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma wordcloud dos principais termos\n",
        "\n",
        "all_text = ' '.join(df['Processed_Abstract'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap=\"twilight_shifted_r\").generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "wordcloud.to_file(\"word_cloud.png\")"
      ],
      "metadata": {
        "id": "oSAXw_MjvwQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Análise de Unigramas/Bigramas/Trigramas"
      ],
      "metadata": {
        "id": "7X8z4PmowpWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cufflinks as cf\n",
        "\n",
        "# Criando função para gerar os top N n-gramas\n",
        "\n",
        "def get_top_n_gram(corpus, n=None, r=None):\n",
        "    vec = CountVectorizer(ngram_range=(r, r), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "metadata": {
        "id": "axvjcVpvlvNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando as top-20 palavras (unigramas)\n",
        "\n",
        "common_words = get_top_n_gram(df['Processed_Abstract'], 20, 1)\n",
        "\n",
        "for word, freq in common_words:\n",
        "    print(f\"{word} ({freq})\")\n",
        "\n",
        "cf.go_offline()\n",
        "cf.set_config_file(offline=False, world_readable=True)\n",
        "\n",
        "df4 = pd.DataFrame(common_words, columns = ['Processed_Abstract' , 'count'])\n",
        "df4.groupby('Processed_Abstract').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', color='red', yTitle='Quantidade', linecolor='black', title='Top 20 unigramas', asFigure=True).show(renderer=\"colab\")\n"
      ],
      "metadata": {
        "id": "t3V_T2evl0Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando os top-20 conjuntos de 2 palavras (bigramas)\n",
        "\n",
        "common_words = get_top_n_gram(df['Processed_Abstract'], 20, 2)\n",
        "\n",
        "for word, freq in common_words:\n",
        "    print(f\"{word} ({freq})\")\n",
        "\n",
        "cf.go_offline()\n",
        "cf.set_config_file(offline=False, world_readable=True)\n",
        "\n",
        "df4 = pd.DataFrame(common_words, columns = ['Processed_Abstract' , 'count'])\n",
        "df4.groupby('Processed_Abstract').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', color='blue', yTitle='Quantidade', linecolor='black', title='Top 20 bigramas', asFigure=True).show(renderer=\"colab\")\n"
      ],
      "metadata": {
        "id": "ecggGBIWwZ2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando os top-20 conjuntos de 3 palavras (trigramas)\n",
        "\n",
        "common_words = get_top_n_gram(df['Processed_Abstract'], 20, 3)\n",
        "\n",
        "for word, freq in common_words:\n",
        "    print(f\"{word} ({freq})\")\n",
        "\n",
        "cf.go_offline()\n",
        "cf.set_config_file(offline=False, world_readable=True)\n",
        "\n",
        "df4 = pd.DataFrame(common_words, columns = ['Processed_Abstract' , 'count'])\n",
        "df4.groupby('Processed_Abstract').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', yTitle='Quantidade', linecolor='black', title='Top 20 trigramas', asFigure=True).show(renderer=\"colab\")\n"
      ],
      "metadata": {
        "id": "UXZrZJxjqofK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA Analysis"
      ],
      "metadata": {
        "id": "6YFN3_uPw9Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise utilizando modelagem de tópicos\n",
        "#https://blog.devgenius.io/working-with-pyldavis-topic-modeling-exploration-tool-b03682d57079\n",
        "\n",
        "!pip install pyldavis\n",
        "!pip install pandas==1.5.1"
      ],
      "metadata": {
        "id": "NxF_RC_1w_Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from gensim import matutils\n",
        "from gensim.corpora import Dictionary\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "from pprint import pprint\n",
        "\n",
        "# Criando um vetorizador de bag-of-words\n",
        "# Cortamos palavras de baixa e alta frequência e filtramos stopwords em inglês\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=10, max_df=0.5, stop_words=\"english\")\n",
        "dtm = vectorizer.fit_transform(df['Processed_Abstract'])\n",
        "\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Aplicando a função de tokenização a cada resumo\n",
        "df['Tokenized_Abstract'] = df['Processed_Abstract'].apply(tokenize)\n",
        "\n",
        "# Criando um dicionário\n",
        "id2word = Dictionary(df['Tokenized_Abstract'])\n",
        "\n",
        "# Criando um corpus\n",
        "corpus = [id2word.doc2bow(tokens) for tokens in df['Tokenized_Abstract']]"
      ],
      "metadata": {
        "id": "HILJe0Mexsnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "# Construindo LDA model\n",
        "\n",
        "num_topics = 5\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                   id2word=id2word,\n",
        "                   num_topics=num_topics,\n",
        "                   iterations=50000,\n",
        "                   random_state=100,\n",
        "                   update_every=1,\n",
        "                   eval_every=10,\n",
        "                   chunksize=100,\n",
        "                   alpha='auto',\n",
        "                   per_word_topics=True)"
      ],
      "metadata": {
        "id": "hfh9ixAtzCyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibindo os principais termos de cada tópico e seu score\n",
        "\n",
        "pprint(lda_model.print_topics(num_words=30))\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "UvgUkBh-zDJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando visualização de distância de tópicos\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
      ],
      "metadata": {
        "id": "QYPfJ67Q5GEB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}